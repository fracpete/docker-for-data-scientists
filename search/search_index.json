{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"From one data scientist to another on how to utilize docker to make your life easier. Any data scientist that ever had to set up an environment for a deep learning framework, knows that getting the combination of CUDA , cuDNN , deep learning framework and other libraries right is a frustrating exercise. With docker you still have to go through the pain of figuring out the right combination, but... And it is a BIG but! Once you have this blueprint called docker image , you can use it other machines as well; you will be up and running in seconds. This is by no means supposed to be an exhaustive introduction to docker (docker can do a lot more!), but merely for getting you started on your journey. Disclaimer: This introduction was written using a Linux host system. If you are using Mac OSX or WSL under Windows, your mileage may vary... To get started with docker, you should look at the following sections: Basics - explains the most common operations that you will need to know Dockerfile - explains the structure of the Dockerfile file which is used to create docker images Best practices - what to keep in mind when creating and using images Repositories - where to find the base images that your own images will use If you should encounter problems, then have a look here: Troubleshooting Just like with any programming language or complex framework, there are certain things that can make your life easier. Therefore do not forget to have a look at: Tips & Tricks Once you got a handle on things, and you are getting tired on manually building images, you might want to look into automating your builds and maybe also run your own registry/proxy. In that case, have a look at the following sections: Automation Registry Finally, if you need to orchestrate multiple docker images, you can have a look at: docker compose","title":"Home"},{"location":"automation/","text":"Building docker images by hand is fine when you are developing a new image. However, once the development phase is over and maintenance starts, you will most likely only make minor changes. Having to build and push out an image then becomes tedious. Instead, automate the build and push process using a build system. Of course, there are a million and one different build tools out there and you will have to find the one that you like best and that fits your environment. Coming from a Java/Maven background, I was already familiar with Jenkins . It may not be the best of interfaces, but if you are only occasionally performing builds then it is nice if you can just parametrize some plugins through a web interface and concentrate on other tasks. For my docker builds, I use the Docker plugin for Jenkins . For images hosted on public registries that use public git repositories, the plugin is really easy to use. Using Jenkins' credential mechanism, you can also push out images to private registries (for cloning/pulling from private git repositories, you can employ some ssh-config magic ). Other (untested) approaches you could look into are: gradle github actions","title":"Automation"},{"location":"basics/","text":"In this section we will touch on a very small subset of functionality provided by the docker command-line tool. With this subset you will be able to use already existing images, spinning up containers and being able to run code from the host machine within containers. Installation If you do not have docker installed yet, then please do so. See the official documentation for your respective operating system flavor. Pulling Before you can use an image, you either need to build it locally or pull it from a registry. The latter is, obviously, done by the pull sub-command. The docker hub allows you to quickly copy the pull command for a specific image (see highlighted area in image below). The URL that you can use with docker consists of these parts: [registry-url/]namespace/image[:tag] If the registry-url is omitted, it defaults to docker.io/ . If :tag is omitted, it defaults to :latest . The command copied from the above page looks like this: docker pull pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel The namespace is pytorch , the image is pytorch and the tag is 1.6.0-cuda10.1-cudnn7-devel . When executing this command, all layers that have not been cached locally will be downloaded. It will look similar to this screen, with each layer having its own unique hash: metal:[101]~>docker pull pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel 1.6.0-cuda10.1-cudnn7-devel: Pulling from pytorch/pytorch 7ddbc47eeb70: Pull complete c1bbdc448b72: Pull complete 8c3b70e39044: Pull complete 45d437916d57: Pull complete d8f1569ddae6: Pull complete 85386706b020: Pull complete ee9b457b77d0: Extracting [==================================================>] 184B/184B be4f3343ecd3: Pulling fs layer 30b4effda4fd: Waiting b398e882f414: Waiting 4fe309685765: Waiting 8b87a3cb3232: Waiting 6cac8a6cf141: Waiting The output from the finished pull will look like this: metal:[103]~>docker pull pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel 1.6.0-cuda10.1-cudnn7-devel: Pulling from pytorch/pytorch 7ddbc47eeb70: Pull complete c1bbdc448b72: Pull complete 8c3b70e39044: Pull complete 45d437916d57: Pull complete d8f1569ddae6: Pull complete 85386706b020: Pull complete ee9b457b77d0: Pull complete be4f3343ecd3: Pull complete 30b4effda4fd: Pull complete b398e882f414: Pull complete 4fe309685765: Pull complete 8b87a3cb3232: Pull complete 6cac8a6cf141: Pull complete Digest: sha256:ccebb46f954b1d32a4700aaeae0e24bd68653f92c6f276a608bf592b660b63d7 Status: Downloaded newer image for pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel docker.io/pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel Running Once an image has been downloaded, you can spin up a container. Think of a container as a concrete instantiation of a blueprint (i.e., the image), which can receive modifications that will stay until its removed. The sub-command for spinning up, is run . When running an image (i.e., spinning up a container), this can be done either in interactive mode or not. The former can be used at development time or for manually running experiments and the latter for a production setting, where you simply supply a command to executed within the container, like building model. For interactive use, you will need the -it flags, which stand for interactive and tty ( T ele TY pewriter or console). For the time being, we will stick with interactive mode. The non-interactive mode is explained briefly in Dockerfile/Running the image (non-interactive) Starting the just downloaded pytorch image in interactive mode is achieved with this command: docker run -it pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel This will give you a prompt similar to this: root@1a3ae8deb7b3:/workspace# Which you can exit like any other shell via exit or CTRL+D . When starting up an image without specifying a name for the container ( --name ), docker will give it an automatic generated name rather than just a hash, which can be quite humorous: pedantic_herschel If you want to list containers, you need to use the action ls of the container sub-command. If all containers, not just currently running ones, should be listed, then you need to add the -a option. So, for listing all containers on your system, run this: docker container ls -a Other useful actions for the container sub-command are: start - start an existing container stop - stop a running container rm - remove a stopped container GPU support The image that we downloaded comes with CUDA support, in order to make use of an NVIDIA GPU that we have on our host system. But, the graphics card does not automatically get made available to the containers, we need to explicitly state that we want a container to have access to it. For docker versions prior to 19.03 (e.g., still used by some NVIDIA IoT devices), you need to supply the following parameter: --runtime=nvidia Otherwise, use this: --gpus=all Or supply the specific ID of the GPU, if you are on a multi-GPU system ( source ): --gpus=1 Volumes By default, a docker container does not have access to any directories on the host system and you will have to explicitly give access via volumes . Docker distinguishes between named volumes (partitions created and managed by the docker daemon) and simply mapped directories. Usually, it is sufficient to just map local directories into your container. That way, you have full control over your data and models on the host system. The easist way to map a directory (or even a single file) is to use the -v or --volume option. The alternative is the --mount option, which gives you greater control (but seems like overkill most of the time for a data scientist). For more information, check out the docker documentation on volumes . One thing to be aware of is that you can hide directories within the container by mapping an external directory onto an existing one, e.g., /usr or /opt . This can have unexpected side effects, like missing libraries, executables, data, etc. It is worthwhile to inspect the internals of a container first, before mapping volumes willy-nilly. The command below maps the /some/where directory of the host to the directory /opt/local within the container: docker run \\ -v /some/where:/opt/local \\ -it pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel Please note that directories need to be absolute paths. If you want to map the current directory into the container, then you can make use of the pwd command like this: docker run \\ -v `pwd`:/opt/local \\ -it pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel Of course, the -v option can be supplied multiple times. Clean up You will soon notice that you are accumulating a large number of containers and images on your system, taking up significant space. Here are some commands that you can use to clean up your system: stop all containers docker stop $(docker ps -a -q) * remove all containers docker rm $(docker ps -a -q) purging all unused or dangling images, containers, volumes, and networks: docker system prune you can be even more aggressive when adding the -a flag: docker system prune -a Login & Credentials When using non-public registries, you will most likely have to perform a login, using the login sub-command (and logout for logging out). By default, docker will store your password only base64 encoded and not encrypted. It is therefore recommended using an external credentials store. For more information, see the docker documentation on the login command .","title":"Basics"},{"location":"basics/#installation","text":"If you do not have docker installed yet, then please do so. See the official documentation for your respective operating system flavor.","title":"Installation"},{"location":"basics/#pulling","text":"Before you can use an image, you either need to build it locally or pull it from a registry. The latter is, obviously, done by the pull sub-command. The docker hub allows you to quickly copy the pull command for a specific image (see highlighted area in image below). The URL that you can use with docker consists of these parts: [registry-url/]namespace/image[:tag] If the registry-url is omitted, it defaults to docker.io/ . If :tag is omitted, it defaults to :latest . The command copied from the above page looks like this: docker pull pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel The namespace is pytorch , the image is pytorch and the tag is 1.6.0-cuda10.1-cudnn7-devel . When executing this command, all layers that have not been cached locally will be downloaded. It will look similar to this screen, with each layer having its own unique hash: metal:[101]~>docker pull pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel 1.6.0-cuda10.1-cudnn7-devel: Pulling from pytorch/pytorch 7ddbc47eeb70: Pull complete c1bbdc448b72: Pull complete 8c3b70e39044: Pull complete 45d437916d57: Pull complete d8f1569ddae6: Pull complete 85386706b020: Pull complete ee9b457b77d0: Extracting [==================================================>] 184B/184B be4f3343ecd3: Pulling fs layer 30b4effda4fd: Waiting b398e882f414: Waiting 4fe309685765: Waiting 8b87a3cb3232: Waiting 6cac8a6cf141: Waiting The output from the finished pull will look like this: metal:[103]~>docker pull pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel 1.6.0-cuda10.1-cudnn7-devel: Pulling from pytorch/pytorch 7ddbc47eeb70: Pull complete c1bbdc448b72: Pull complete 8c3b70e39044: Pull complete 45d437916d57: Pull complete d8f1569ddae6: Pull complete 85386706b020: Pull complete ee9b457b77d0: Pull complete be4f3343ecd3: Pull complete 30b4effda4fd: Pull complete b398e882f414: Pull complete 4fe309685765: Pull complete 8b87a3cb3232: Pull complete 6cac8a6cf141: Pull complete Digest: sha256:ccebb46f954b1d32a4700aaeae0e24bd68653f92c6f276a608bf592b660b63d7 Status: Downloaded newer image for pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel docker.io/pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel","title":"Pulling"},{"location":"basics/#running","text":"Once an image has been downloaded, you can spin up a container. Think of a container as a concrete instantiation of a blueprint (i.e., the image), which can receive modifications that will stay until its removed. The sub-command for spinning up, is run . When running an image (i.e., spinning up a container), this can be done either in interactive mode or not. The former can be used at development time or for manually running experiments and the latter for a production setting, where you simply supply a command to executed within the container, like building model. For interactive use, you will need the -it flags, which stand for interactive and tty ( T ele TY pewriter or console). For the time being, we will stick with interactive mode. The non-interactive mode is explained briefly in Dockerfile/Running the image (non-interactive) Starting the just downloaded pytorch image in interactive mode is achieved with this command: docker run -it pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel This will give you a prompt similar to this: root@1a3ae8deb7b3:/workspace# Which you can exit like any other shell via exit or CTRL+D . When starting up an image without specifying a name for the container ( --name ), docker will give it an automatic generated name rather than just a hash, which can be quite humorous: pedantic_herschel If you want to list containers, you need to use the action ls of the container sub-command. If all containers, not just currently running ones, should be listed, then you need to add the -a option. So, for listing all containers on your system, run this: docker container ls -a Other useful actions for the container sub-command are: start - start an existing container stop - stop a running container rm - remove a stopped container","title":"Running"},{"location":"basics/#gpu-support","text":"The image that we downloaded comes with CUDA support, in order to make use of an NVIDIA GPU that we have on our host system. But, the graphics card does not automatically get made available to the containers, we need to explicitly state that we want a container to have access to it. For docker versions prior to 19.03 (e.g., still used by some NVIDIA IoT devices), you need to supply the following parameter: --runtime=nvidia Otherwise, use this: --gpus=all Or supply the specific ID of the GPU, if you are on a multi-GPU system ( source ): --gpus=1","title":"GPU support"},{"location":"basics/#volumes","text":"By default, a docker container does not have access to any directories on the host system and you will have to explicitly give access via volumes . Docker distinguishes between named volumes (partitions created and managed by the docker daemon) and simply mapped directories. Usually, it is sufficient to just map local directories into your container. That way, you have full control over your data and models on the host system. The easist way to map a directory (or even a single file) is to use the -v or --volume option. The alternative is the --mount option, which gives you greater control (but seems like overkill most of the time for a data scientist). For more information, check out the docker documentation on volumes . One thing to be aware of is that you can hide directories within the container by mapping an external directory onto an existing one, e.g., /usr or /opt . This can have unexpected side effects, like missing libraries, executables, data, etc. It is worthwhile to inspect the internals of a container first, before mapping volumes willy-nilly. The command below maps the /some/where directory of the host to the directory /opt/local within the container: docker run \\ -v /some/where:/opt/local \\ -it pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel Please note that directories need to be absolute paths. If you want to map the current directory into the container, then you can make use of the pwd command like this: docker run \\ -v `pwd`:/opt/local \\ -it pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel Of course, the -v option can be supplied multiple times.","title":"Volumes"},{"location":"basics/#clean-up","text":"You will soon notice that you are accumulating a large number of containers and images on your system, taking up significant space. Here are some commands that you can use to clean up your system: stop all containers docker stop $(docker ps -a -q) * remove all containers docker rm $(docker ps -a -q) purging all unused or dangling images, containers, volumes, and networks: docker system prune you can be even more aggressive when adding the -a flag: docker system prune -a","title":"Clean up"},{"location":"basics/#login-credentials","text":"When using non-public registries, you will most likely have to perform a login, using the login sub-command (and logout for logging out). By default, docker will store your password only base64 encoded and not encrypted. It is therefore recommended using an external credentials store. For more information, see the docker documentation on the login command .","title":"Login &amp; Credentials"},{"location":"best_practices/","text":"Versioning Since docker determines by the hash of a command whether this particular layer needs rebuilding, you should, wherever possible, specify the version of the library that you are installing. That way, whenever you upgrade a library, that layer (and all subsequent ones) will get rebuilt. The added bonus is that you will not accidentally rebuild the image with a newer version of a library at a later stage that may now be incompatible with all the others ( Yeah, I'm looking at you, numpy !). So instead of: RUN python3 -m pip install --no-cache-dir numpy && \\ ... You should do something like this: RUN python3 -m pip install --no-cache-dir numpy==1.17.4 && \\ ... Of course, when you are cloning directly from a github repository, because you require a specific bugfix or the library does not offer any releases (or only very infrequent), then you should use a specific commit hash in your command: RUN git clone https://github.com/ACCOUNT/REPO.git && \\ cd REPO && \\ git reset --hard 11223344556677889900AABBCCDDEEFF11223344 && \\ ... Docker group Instead of running the docker command via sudo , you should consider adding your user to the docker group instead (in /etc/group ). That way, you can run the docker command as a regular user. Launch container as regular user Once development of your docker image has finished, you should avoid running docker containers as the root user and instead run it as the current user (which also avoids creating output files in volumes that can only be removed by root ). Use the -u and -e command to specify the user/group IDs and the user name as follows: docker run -u $(id -u):$(id -g) -e USER=$USER ... However, the environment variables for you command prompt (when you are using your container in interactive mode) may not be able to handle this properly. In such a case you will get output similar to this: groups: cannot find name for group ID XYZ I have no name!@c7355d9a1b59:/$ You can rectify this by creating a custom bash.bashrc file using the docker-banner-gen library (which not only outputs a nice banner, but also warns you in case you are running the container as root ). This custom file can then be added to your docker image using the following command: COPY bash.bashrc /etc/bash.bashrc No Anaconda Instead of using Anaconda for installing Python packages, you can just use plain pip to install packages from the Python Package Index . Anaconda in itself is quite a large installation and will increase the overall docker image size unnecessarily. Clean up pip Remove the pip cache to reduce the size of your layer after you installed all your packages: rm -Rf /root/.cache/pip Alternatively, do not cache downloads when installing a package: pip install --no-cache-dir ... Clean up apt After performing installs via apt or apt-get , clean up the cache to reduce the size of your layer: apt-get clean && \\ rm -rf /var/lib/apt/lists/* Caching models As soon as you stop/remove the container, all modifications will be lost. This also includes any pretrained networks that any of the deep learning frameworks downloaded into its cache. In order to avoid the constant downloads, you can map the cache directories for the relevant framework to a local directory on the host: PyTorch when running the container as root : -v /some/where/cache:/root/.torch \\ when running the container as regular user: -v /some/where/cache:/.torch \\ Keras Map the cache directory ( /root/.keras ) when running container as root : -v /some/where/cache:/root/.keras Or when running as regular user ( /tmp/.keras ): -v /some/where/cache:/tmp/.keras","title":"Best practices"},{"location":"best_practices/#versioning","text":"Since docker determines by the hash of a command whether this particular layer needs rebuilding, you should, wherever possible, specify the version of the library that you are installing. That way, whenever you upgrade a library, that layer (and all subsequent ones) will get rebuilt. The added bonus is that you will not accidentally rebuild the image with a newer version of a library at a later stage that may now be incompatible with all the others ( Yeah, I'm looking at you, numpy !). So instead of: RUN python3 -m pip install --no-cache-dir numpy && \\ ... You should do something like this: RUN python3 -m pip install --no-cache-dir numpy==1.17.4 && \\ ... Of course, when you are cloning directly from a github repository, because you require a specific bugfix or the library does not offer any releases (or only very infrequent), then you should use a specific commit hash in your command: RUN git clone https://github.com/ACCOUNT/REPO.git && \\ cd REPO && \\ git reset --hard 11223344556677889900AABBCCDDEEFF11223344 && \\ ...","title":"Versioning"},{"location":"best_practices/#docker-group","text":"Instead of running the docker command via sudo , you should consider adding your user to the docker group instead (in /etc/group ). That way, you can run the docker command as a regular user.","title":"Docker group"},{"location":"best_practices/#launch-container-as-regular-user","text":"Once development of your docker image has finished, you should avoid running docker containers as the root user and instead run it as the current user (which also avoids creating output files in volumes that can only be removed by root ). Use the -u and -e command to specify the user/group IDs and the user name as follows: docker run -u $(id -u):$(id -g) -e USER=$USER ... However, the environment variables for you command prompt (when you are using your container in interactive mode) may not be able to handle this properly. In such a case you will get output similar to this: groups: cannot find name for group ID XYZ I have no name!@c7355d9a1b59:/$ You can rectify this by creating a custom bash.bashrc file using the docker-banner-gen library (which not only outputs a nice banner, but also warns you in case you are running the container as root ). This custom file can then be added to your docker image using the following command: COPY bash.bashrc /etc/bash.bashrc","title":"Launch container as regular user"},{"location":"best_practices/#no-anaconda","text":"Instead of using Anaconda for installing Python packages, you can just use plain pip to install packages from the Python Package Index . Anaconda in itself is quite a large installation and will increase the overall docker image size unnecessarily.","title":"No Anaconda"},{"location":"best_practices/#clean-up-pip","text":"Remove the pip cache to reduce the size of your layer after you installed all your packages: rm -Rf /root/.cache/pip Alternatively, do not cache downloads when installing a package: pip install --no-cache-dir ...","title":"Clean up pip"},{"location":"best_practices/#clean-up-apt","text":"After performing installs via apt or apt-get , clean up the cache to reduce the size of your layer: apt-get clean && \\ rm -rf /var/lib/apt/lists/*","title":"Clean up apt"},{"location":"best_practices/#caching-models","text":"As soon as you stop/remove the container, all modifications will be lost. This also includes any pretrained networks that any of the deep learning frameworks downloaded into its cache. In order to avoid the constant downloads, you can map the cache directories for the relevant framework to a local directory on the host: PyTorch when running the container as root : -v /some/where/cache:/root/.torch \\ when running the container as regular user: -v /some/where/cache:/.torch \\ Keras Map the cache directory ( /root/.keras ) when running container as root : -v /some/where/cache:/root/.keras Or when running as regular user ( /tmp/.keras ): -v /some/where/cache:/tmp/.keras","title":"Caching models"},{"location":"dockerfile/","text":"Using existing docker images unfortunately only gets you so far. Especially, when you need to put models into production, it is very likely that you will need to encapsulate more logic within the image itself to integrate it in your existing processes. In this section, you will learn the basic building blocks for generating docker images. By convention the instructions for a docker image are stored in a text file called Dockerfile ( full reference ). This is not set in stone, so you can name it anything you like. However, it is usually best to stick with the conventions that a community expects (just like with PEP-8 in the Python community). In order to make this section not just a hello world example, we will build an image that we can use for training and evaluating a PyTorch model on the CIFAR10 challenge . The Python script will also output a Matplotlib bar chart, displaying the accuracies of the classes. Each of the following sections will introduce a docker command that will make up the final docker script for creating the image. Of course, we will also build and use the image, as well as push it out to make it publicly available. Comments Before we dive into the actual docker commands, a quick note on comments. Just like with any other programming language, it pays to comment your instructions. Especially, if you need to work around quirks with some strange command-lines. Comments in docker files, like with bash programming, are line comments and start with # . FROM Docker images (just like ogres) are like onions, consisting of multiple layers. This makes it easy to add more functionality to existing images: reusing is better than recreating. This approach also preserves a lot of space. The first (non-comment) statement in your Dockerfile needs to be the FROM statement, which tells docker the particular base image you want to build on top. Reusing the image from our pull command in the Basics section, we get the following initial statement: FROM pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel ARG In some cases, your Dockerfile script might need to be parametrized for the build from the outside. For defining such a build parameter (with a default value), you can use the ARG instruction. Using the --build-arg command-line you can override the default value. The ARG syntax is simple: ARG key=value You can also use ARG to make your FROM statement easier to read, but be aware of the interaction between FROM and ARG . Using ARG , we can pull out the version numbers from our original FROM statement and put them in variables. Variables can be used in other statements via ${...} . Here is the human-readable FROM statement: ARG PYTORCH=\"1.6.0\" ARG CUDA=\"10.1\" ARG CUDNN=\"7\" FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel ENV In contrast to ARG , the ENV instruction is persistent within the docker image and can be used to set environment variables rather than build variables. If the variable should not be persistent in the final image, as it might interfere with subsequent layers, then you should consider using ARG instead. One such environment variable is DEBIAN_FRONTEND , which changes the behavior of apt-get (Debian tool to install packages): ENV DEBIAN_FRONTEND=noninteractive The docker documentation on the ENV command lists alternative ways of specifying such an environment variable, for limiting the scope. In the Interactive timezone prompt section section, you read why you might want to use this variable. RUN Once you have set the stage with base image and variables, you can set about installing the required software packages. For Debian/Ubuntu based systems, this usually involves invoking the apt-get package manager and for Python pip/pip3 . Executing commands is achieved with the RUN command. For installing matplotlib, which is not part of our base image, we can use: RUN pip --no-cache install matplotlib You will notice the --no-cache flag, which is not something you would normally use. In order to avoid downloading the same library over and over again, pip caches them by default. This is not necessary for a docker image and just takes up more space. But before you argue that you could just remove unwanted files at the end of your docker image then you do not take into account that docker works in layers: every command is basically a layer. Though a subsequent layer may remove files (and you will not see them in the final image), they are still present in the layer that they got introduced in. Long story short: either always clean up within the same layer or avoid generating temporary files altogether. Check out the following Best practices sections: Clean up pip Clean up apt COPY Quite often, you will end up just needing little scripts within a docker image to do the work for you (since all the libraries get installed via apt-get or pip ). For copying files or directories, you can use the COPY command. One thing that we have not talked about yet is the docker context . This context includes all the files and directories that are on the same level as the Dockerfile . The COPY command can only use files and directories that are within this context, but not outside (e.g., going up in the directory structure). You also need to be aware that the complete docker context will get sent to the docker daemon during the build process. So best not to have any unnecessary files and directories in the same directory. However, if it cannot be avoided for certain files/directories to be present you can use the .dockerignore file to exclude them. As mentioned at the start, we want to train a PyTorch model and evaluate it. Download the test.py script and place it next to the Dockerfile that you are currently working on. In order to include this Python script, use the following command: COPY test.py /opt/test/test.py The COPY command will automatically create directories if they are not present, which is /opt/test in our case. Any existing file will get overwritten as well. Not only Python scripts can be copied, you can also create executable bash scripts that call your actual Python scripts and place them in /usr/bin . For our test.py the bash script would look like this: #!/bin/bash python /opt/test/test.py If your script supports command-line options, you can pass them through using \"$@\" : #!/bin/bash python /opt/test/test.py \"$@\" WORKDIR With the WOKRDIR command, you can change the current working directory within your docker script. If the directory does not exist yet, it will get created automatically. This eliminates the need to use mkdir commands. In our case, we can just use the command to make the docker container automatically start the prompt in the directory of our Python script ( /opt/test ) when used in interactive mode: WORKDIR /opt/test Other useful commands ADD ENTRYPOINT Building the image With our Dockerfile now finally complete, we are ready to kick off a build. After changing into the directory containing the Dockerfile , you can use the build sub-command to perform the build. Rather than using a hash, we can give it a name via the -t option ( tagging it): docker build -t pytorchtest . You should see similar output like the one below: Sending build context to Docker daemon 260.6kB Step 1/8 : ARG PYTORCH=\"1.6.0\" Step 2/8 : ARG CUDA=\"10.1\" Step 3/8 : ARG CUDNN=\"7\" Step 4/8 : FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel ---> bb833e4d631f Step 5/8 : ENV DEBIAN_FRONTEND noninteractive ---> Running in 71812ab36f5a Removing intermediate container 71812ab36f5a ---> 77a27d586581 Step 6/8 : RUN pip --no-cache install matplotlib ---> Running in 10d85cc41a73 Collecting matplotlib Downloading matplotlib-3.4.2-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB) Collecting kiwisolver>=1.0.1 Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB) Collecting cycler>=0.10 Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB) Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (7.2.0) Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.18.5) Collecting python-dateutil>=2.7 Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB) Collecting pyparsing>=2.2.1 Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB) Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0) Installing collected packages: kiwisolver, cycler, python-dateutil, pyparsing, matplotlib Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.4.2 pyparsing-2.4.7 python-dateutil-2.8.1 Removing intermediate container 10d85cc41a73 ---> 0361d66c3c44 Step 7/8 : WORKDIR /opt/test ---> Running in 14568b3fead5 Removing intermediate container 14568b3fead5 ---> 7c1a8b7229d0 Step 8/8 : COPY test.py /opt/test/test.py ---> ed55c4fb8e62 Successfully built ed55c4fb8e62 Successfully tagged pytorchtest:latest Running the image (interactive) With the image successfully built, you can now use it. For this you need to employ the RUN command. docker run --gpus=all -v `pwd`:/opt/local -it pytorchtest Once the prompt appears, you can execute the test.py script: root@5e2245aa020a:/opt/test# python test.py While the script is executing, you should see output like this: cuda:0 Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 170369024/170498071 [00:13<00:00, 13670749.69it/s]Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified [1, 2000] loss: 2.173 [1, 4000] loss: 1.815 [1, 6000] loss: 1.659 170500096it [00:30, 13670749.69it/s] [1, 8000] loss: 1.557 [1, 10000] loss: 1.499 [1, 12000] loss: 1.449 [2, 2000] loss: 1.377 [2, 4000] loss: 1.356 [2, 6000] loss: 1.329 [2, 8000] loss: 1.315 [2, 10000] loss: 1.293 [2, 12000] loss: 1.264 Finished Training GroundTruth: cat ship ship plane Predicted: cat ship car plane Accuracy of the network on the 10000 test images: 55 % Accuracy of plane : 60 % Accuracy of car : 65 % Accuracy of bird : 29 % Accuracy of cat : 44 % Accuracy of deer : 54 % Accuracy of dog : 41 % Accuracy of frog : 52 % Accuracy of horse : 68 % Accuracy of ship : 78 % Accuracy of truck : 63 % 170500096it [01:16, 2240182.93it/s] At the end, the script will generate a bar chart plot and save it as /opt/local/figure.png : If you look at the permissions of the generated image, you will notice that the owner is root . Depending on your permissions on the host system, you might not be able to remove it from outside the container. One approach is to change the owner using chown (from within the container), but that can become tedious. Instead, see the following Best practices section on how to best address this: Launch container as regular user Congratulations, you have assembled, built and run your first docker image! Running the image (non-interactive) Of course, you do not have to run the image interactively at all. After the initial development of your docker image and code, you can then use it in your production system. For running it in non-interactive mode, simply remove the -it flags and append the command that you want to run. In our case, this is: python3 /opt/test/test.py The full command-line therefore looks like: docker run --gpus=all -v `pwd`:/opt/local pytorchtest python3 /opt/test/test.py Optimizing an image As a final note, you should consider revisiting your Dockerfile once you have verified that everything is working and optimize it. Optimizing entails: Combine apt-get commands in a single RUN and remove caches at the end Combine pip commands in a single RUN run and make sure that the pip cache is removed Pushing the image Once you are happy with your image and you want to use it on another machine, you will need to push it out to a registry. Otherwise, you will not be able to use the image on another machine without having to re-build it (therefore defeating the purpose of reusable images). For pushing an image, there are typically two sub-commands that come into play: tag push When building images locally, as we did above, the name is fairly irrelevant ( pytorchtest ). However, when pushing an image to a registry (docker hub or your own ), then naming (aka tagging ) an image requires a bit more thought. Assuming that you have a user account on docker hub called user1234 , then you could name your image like this: user1234/pytorchtest:pytorch1.6.0-cuda10.1-cudnn7-devel-0.0.1 That way, you still keep using your local image name pytorchtest , but you also include the version of PyTorch, CUDA and cuDNN. The 0.0.1 at the end, is the actual version of your image. In order to push the image pytorchtest out to docker hub, you first need to give it the proper tag: docker tag \\ pytorchtest \\ user1234/pytorchtest:pytorch1.6.0-cuda10.1-cudnn7-devel-0.0.1 And for pushing it out, use this command: docker push user1234/pytorchtest:pytorch1.6.0-cuda10.1-cudnn7-devel-0.0.1 Once the push is complete, you will find this image at the following URL: https://hub.docker.com/u/user1234 Depending on the number of images you have, you may have to search for the pytorchtest:pytorch1.6.0-cuda10.1-cudnn7-devel-0.0.1 tag.","title":"Dockerfile"},{"location":"dockerfile/#comments","text":"Before we dive into the actual docker commands, a quick note on comments. Just like with any other programming language, it pays to comment your instructions. Especially, if you need to work around quirks with some strange command-lines. Comments in docker files, like with bash programming, are line comments and start with # .","title":"Comments"},{"location":"dockerfile/#from","text":"Docker images (just like ogres) are like onions, consisting of multiple layers. This makes it easy to add more functionality to existing images: reusing is better than recreating. This approach also preserves a lot of space. The first (non-comment) statement in your Dockerfile needs to be the FROM statement, which tells docker the particular base image you want to build on top. Reusing the image from our pull command in the Basics section, we get the following initial statement: FROM pytorch/pytorch:1.6.0-cuda10.1-cudnn7-devel","title":"FROM"},{"location":"dockerfile/#arg","text":"In some cases, your Dockerfile script might need to be parametrized for the build from the outside. For defining such a build parameter (with a default value), you can use the ARG instruction. Using the --build-arg command-line you can override the default value. The ARG syntax is simple: ARG key=value You can also use ARG to make your FROM statement easier to read, but be aware of the interaction between FROM and ARG . Using ARG , we can pull out the version numbers from our original FROM statement and put them in variables. Variables can be used in other statements via ${...} . Here is the human-readable FROM statement: ARG PYTORCH=\"1.6.0\" ARG CUDA=\"10.1\" ARG CUDNN=\"7\" FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel","title":"ARG"},{"location":"dockerfile/#env","text":"In contrast to ARG , the ENV instruction is persistent within the docker image and can be used to set environment variables rather than build variables. If the variable should not be persistent in the final image, as it might interfere with subsequent layers, then you should consider using ARG instead. One such environment variable is DEBIAN_FRONTEND , which changes the behavior of apt-get (Debian tool to install packages): ENV DEBIAN_FRONTEND=noninteractive The docker documentation on the ENV command lists alternative ways of specifying such an environment variable, for limiting the scope. In the Interactive timezone prompt section section, you read why you might want to use this variable.","title":"ENV"},{"location":"dockerfile/#run","text":"Once you have set the stage with base image and variables, you can set about installing the required software packages. For Debian/Ubuntu based systems, this usually involves invoking the apt-get package manager and for Python pip/pip3 . Executing commands is achieved with the RUN command. For installing matplotlib, which is not part of our base image, we can use: RUN pip --no-cache install matplotlib You will notice the --no-cache flag, which is not something you would normally use. In order to avoid downloading the same library over and over again, pip caches them by default. This is not necessary for a docker image and just takes up more space. But before you argue that you could just remove unwanted files at the end of your docker image then you do not take into account that docker works in layers: every command is basically a layer. Though a subsequent layer may remove files (and you will not see them in the final image), they are still present in the layer that they got introduced in. Long story short: either always clean up within the same layer or avoid generating temporary files altogether. Check out the following Best practices sections: Clean up pip Clean up apt","title":"RUN"},{"location":"dockerfile/#copy","text":"Quite often, you will end up just needing little scripts within a docker image to do the work for you (since all the libraries get installed via apt-get or pip ). For copying files or directories, you can use the COPY command. One thing that we have not talked about yet is the docker context . This context includes all the files and directories that are on the same level as the Dockerfile . The COPY command can only use files and directories that are within this context, but not outside (e.g., going up in the directory structure). You also need to be aware that the complete docker context will get sent to the docker daemon during the build process. So best not to have any unnecessary files and directories in the same directory. However, if it cannot be avoided for certain files/directories to be present you can use the .dockerignore file to exclude them. As mentioned at the start, we want to train a PyTorch model and evaluate it. Download the test.py script and place it next to the Dockerfile that you are currently working on. In order to include this Python script, use the following command: COPY test.py /opt/test/test.py The COPY command will automatically create directories if they are not present, which is /opt/test in our case. Any existing file will get overwritten as well. Not only Python scripts can be copied, you can also create executable bash scripts that call your actual Python scripts and place them in /usr/bin . For our test.py the bash script would look like this: #!/bin/bash python /opt/test/test.py If your script supports command-line options, you can pass them through using \"$@\" : #!/bin/bash python /opt/test/test.py \"$@\"","title":"COPY"},{"location":"dockerfile/#workdir","text":"With the WOKRDIR command, you can change the current working directory within your docker script. If the directory does not exist yet, it will get created automatically. This eliminates the need to use mkdir commands. In our case, we can just use the command to make the docker container automatically start the prompt in the directory of our Python script ( /opt/test ) when used in interactive mode: WORKDIR /opt/test","title":"WORKDIR"},{"location":"dockerfile/#other-useful-commands","text":"ADD ENTRYPOINT","title":"Other useful commands"},{"location":"dockerfile/#building-the-image","text":"With our Dockerfile now finally complete, we are ready to kick off a build. After changing into the directory containing the Dockerfile , you can use the build sub-command to perform the build. Rather than using a hash, we can give it a name via the -t option ( tagging it): docker build -t pytorchtest . You should see similar output like the one below: Sending build context to Docker daemon 260.6kB Step 1/8 : ARG PYTORCH=\"1.6.0\" Step 2/8 : ARG CUDA=\"10.1\" Step 3/8 : ARG CUDNN=\"7\" Step 4/8 : FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel ---> bb833e4d631f Step 5/8 : ENV DEBIAN_FRONTEND noninteractive ---> Running in 71812ab36f5a Removing intermediate container 71812ab36f5a ---> 77a27d586581 Step 6/8 : RUN pip --no-cache install matplotlib ---> Running in 10d85cc41a73 Collecting matplotlib Downloading matplotlib-3.4.2-cp37-cp37m-manylinux1_x86_64.whl (10.3 MB) Collecting kiwisolver>=1.0.1 Downloading kiwisolver-1.3.1-cp37-cp37m-manylinux1_x86_64.whl (1.1 MB) Collecting cycler>=0.10 Downloading cycler-0.10.0-py2.py3-none-any.whl (6.5 kB) Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (7.2.0) Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.7/site-packages (from matplotlib) (1.18.5) Collecting python-dateutil>=2.7 Downloading python_dateutil-2.8.1-py2.py3-none-any.whl (227 kB) Collecting pyparsing>=2.2.1 Downloading pyparsing-2.4.7-py2.py3-none-any.whl (67 kB) Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0) Installing collected packages: kiwisolver, cycler, python-dateutil, pyparsing, matplotlib Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.4.2 pyparsing-2.4.7 python-dateutil-2.8.1 Removing intermediate container 10d85cc41a73 ---> 0361d66c3c44 Step 7/8 : WORKDIR /opt/test ---> Running in 14568b3fead5 Removing intermediate container 14568b3fead5 ---> 7c1a8b7229d0 Step 8/8 : COPY test.py /opt/test/test.py ---> ed55c4fb8e62 Successfully built ed55c4fb8e62 Successfully tagged pytorchtest:latest","title":"Building the image"},{"location":"dockerfile/#running-the-image-interactive","text":"With the image successfully built, you can now use it. For this you need to employ the RUN command. docker run --gpus=all -v `pwd`:/opt/local -it pytorchtest Once the prompt appears, you can execute the test.py script: root@5e2245aa020a:/opt/test# python test.py While the script is executing, you should see output like this: cuda:0 Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2589| 170369024/170498071 [00:13<00:00, 13670749.69it/s]Extracting ./data/cifar-10-python.tar.gz to ./data Files already downloaded and verified [1, 2000] loss: 2.173 [1, 4000] loss: 1.815 [1, 6000] loss: 1.659 170500096it [00:30, 13670749.69it/s] [1, 8000] loss: 1.557 [1, 10000] loss: 1.499 [1, 12000] loss: 1.449 [2, 2000] loss: 1.377 [2, 4000] loss: 1.356 [2, 6000] loss: 1.329 [2, 8000] loss: 1.315 [2, 10000] loss: 1.293 [2, 12000] loss: 1.264 Finished Training GroundTruth: cat ship ship plane Predicted: cat ship car plane Accuracy of the network on the 10000 test images: 55 % Accuracy of plane : 60 % Accuracy of car : 65 % Accuracy of bird : 29 % Accuracy of cat : 44 % Accuracy of deer : 54 % Accuracy of dog : 41 % Accuracy of frog : 52 % Accuracy of horse : 68 % Accuracy of ship : 78 % Accuracy of truck : 63 % 170500096it [01:16, 2240182.93it/s] At the end, the script will generate a bar chart plot and save it as /opt/local/figure.png : If you look at the permissions of the generated image, you will notice that the owner is root . Depending on your permissions on the host system, you might not be able to remove it from outside the container. One approach is to change the owner using chown (from within the container), but that can become tedious. Instead, see the following Best practices section on how to best address this: Launch container as regular user Congratulations, you have assembled, built and run your first docker image!","title":"Running the image (interactive)"},{"location":"dockerfile/#running-the-image-non-interactive","text":"Of course, you do not have to run the image interactively at all. After the initial development of your docker image and code, you can then use it in your production system. For running it in non-interactive mode, simply remove the -it flags and append the command that you want to run. In our case, this is: python3 /opt/test/test.py The full command-line therefore looks like: docker run --gpus=all -v `pwd`:/opt/local pytorchtest python3 /opt/test/test.py","title":"Running the image (non-interactive)"},{"location":"dockerfile/#optimizing-an-image","text":"As a final note, you should consider revisiting your Dockerfile once you have verified that everything is working and optimize it. Optimizing entails: Combine apt-get commands in a single RUN and remove caches at the end Combine pip commands in a single RUN run and make sure that the pip cache is removed","title":"Optimizing an image"},{"location":"dockerfile/#pushing-the-image","text":"Once you are happy with your image and you want to use it on another machine, you will need to push it out to a registry. Otherwise, you will not be able to use the image on another machine without having to re-build it (therefore defeating the purpose of reusable images). For pushing an image, there are typically two sub-commands that come into play: tag push When building images locally, as we did above, the name is fairly irrelevant ( pytorchtest ). However, when pushing an image to a registry (docker hub or your own ), then naming (aka tagging ) an image requires a bit more thought. Assuming that you have a user account on docker hub called user1234 , then you could name your image like this: user1234/pytorchtest:pytorch1.6.0-cuda10.1-cudnn7-devel-0.0.1 That way, you still keep using your local image name pytorchtest , but you also include the version of PyTorch, CUDA and cuDNN. The 0.0.1 at the end, is the actual version of your image. In order to push the image pytorchtest out to docker hub, you first need to give it the proper tag: docker tag \\ pytorchtest \\ user1234/pytorchtest:pytorch1.6.0-cuda10.1-cudnn7-devel-0.0.1 And for pushing it out, use this command: docker push user1234/pytorchtest:pytorch1.6.0-cuda10.1-cudnn7-devel-0.0.1 Once the push is complete, you will find this image at the following URL: https://hub.docker.com/u/user1234 Depending on the number of images you have, you may have to search for the pytorchtest:pytorch1.6.0-cuda10.1-cudnn7-devel-0.0.1 tag.","title":"Pushing the image"},{"location":"registry/","text":"If you have to manage internal images as well, that should not be available from the outside (e.g., via docker hub), then you should consider running your own in-house registry. Nexus If your setup requires access control or you just want to have a web interface, then I can recommend Sonatype's Nexus: https://www.sonatype.com/products/repository-oss NB: The open-source version is absolutely sufficient for this purpose ( OSS vs Pro ). The biggest advantage of having a local Nexus instance is, that you can use it as a proxy for docker hub. That way, you can pull base images from your local network rather than having to pull them from a distant server. Using a proxy makes rebuilding images very fast, especially if you just removed all images from your machine or moved to a new machine. Example Assuming your registry myregistry.example.com is available via https (port 443), then you can change the FROM directive in your Dockerfile from: ARG PYTORCH=\"1.6.0\" ARG CUDA=\"10.1\" ARG CUDNN=\"7\" FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel To this: ARG PYTORCH=\"1.6.0\" ARG CUDA=\"10.1\" ARG CUDNN=\"7\" ARG DOCKER_REGISTRY=myregistry.example.com:443/ FROM ${DOCKER_REGISTRY}pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel Docker will then pull the base image through your registry (i.e., your proxy) instead of directly from docker hub. Also, that way you can still influence the DOCKER_REGISTRY value via the --build-arg command-line option.","title":"Registry"},{"location":"registry/#nexus","text":"If your setup requires access control or you just want to have a web interface, then I can recommend Sonatype's Nexus: https://www.sonatype.com/products/repository-oss NB: The open-source version is absolutely sufficient for this purpose ( OSS vs Pro ). The biggest advantage of having a local Nexus instance is, that you can use it as a proxy for docker hub. That way, you can pull base images from your local network rather than having to pull them from a distant server. Using a proxy makes rebuilding images very fast, especially if you just removed all images from your machine or moved to a new machine.","title":"Nexus"},{"location":"registry/#example","text":"Assuming your registry myregistry.example.com is available via https (port 443), then you can change the FROM directive in your Dockerfile from: ARG PYTORCH=\"1.6.0\" ARG CUDA=\"10.1\" ARG CUDNN=\"7\" FROM pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel To this: ARG PYTORCH=\"1.6.0\" ARG CUDA=\"10.1\" ARG CUDNN=\"7\" ARG DOCKER_REGISTRY=myregistry.example.com:443/ FROM ${DOCKER_REGISTRY}pytorch/pytorch:${PYTORCH}-cuda${CUDA}-cudnn${CUDNN}-devel Docker will then pull the base image through your registry (i.e., your proxy) instead of directly from docker hub. Also, that way you can still influence the DOCKER_REGISTRY value via the --build-arg command-line option.","title":"Example"},{"location":"repos/","text":"A lot of open-source deep learning projects will supply docker base images already on docker hub , which will make your life much easier. By using these base images, you will only need to install additional libraries that your own project requires. Here is a (non-exhaustive) list of repositories on docker hub: Ubuntu CUDA PyTorch (tend to use Anaconda, unfortunately) Tensorflow Of course, NVIDIA likes to keep things close to its heart and offers their own registry from which you can also retrieve images for the various IoT devices that NVIDIA has on offer: NGC catalog NVIDIA L4T Base NVIDIA L4T PyTorch NVIDIA L4T Tensorflow","title":"Repositories"},{"location":"tips_and_tricks/","text":"Remove unwanted files Especially during development, it can happen that files get generated when running your docker container that can only be removed by the root user again. If you do not have admin permissions (e.g., via sudo ) on the machine then you can revert to using docker to remove them again: change into the directory with unwanted files and/or directories the following docker command will map the current directory to /opt/local : docker run -v `pwd`:/opt/local -it bash:5.1.8 change into /opt/local and remove all files/dirs (or just the files that need removing): cd /opt/local rm -Rf * Alternative Python version In case your base Debian/Ubuntu distribution does not have the right Python version available, you can get additional versions by adding the deadsnakes ppa : add-apt-repository -y ppa:deadsnakes/ppa && \\ apt-get update Switch default Python version If your base distro has an older version of Python and you do not want to sprinkle the specific newer version throughout your Dockerfile , then you can use update-alternatives on Debian systems to change the default. The following commands switch from Python 3.5 to Python 3.6 for the python3 executable and also use Python 3.6 as the default for the python executable: update-alternatives --install /usr/bin/python python /usr/bin/python3.5 1 && \\ update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2 && \\ update-alternatives --set python /usr/bin/python3.6 && \\ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 1 && \\ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 2 && \\ update-alternatives --set python3 /usr/bin/python3.6 && \\ Screen The screen command-line utility allows you to pick up remote sessions after detaching from them. This is especially helpful when running docker in interactive mode via ssh sessions. If such an ssh session should accidentally close (e.g., internet connection lost, laptop closed), then the docker command would terminate as well. On the remote host, start the screen command before you launch your docker command (you can run multiple screen instances as well) and skip the screen with the licenses etc using Enter or Space . Now you can start up the actual command. If you want to run multiple screen sessions on the same host, then you should name them using the -S sessionname option to easily distinguish them. For detaching a session, use the CTRL+A+D key combination. This will leave your process running in the background and you can close the remote connection. In order to reconnect , simply ssh into the remote host and run screen -r . If there is only one screen session running, then it will automatically reconnect. Otherwise, it will output a list of available sessions. Supply the name of the session that you want to reattach to the -r option. You can exit a screen session by either typing exit or using CTRL+D (just like with a bash shell). Default runtime Instead of always having to type --runtime=nvidia or --gpus=all , you can simply define the default runtime to use with your docker commands ( source ): edit the /etc/docker/daemon.json file as root user insert the key default-runtime with value nvidia so that it looks something like this: { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } } } save the file restart the docker daemon (e.g., sudo systemctl restart docker ) Thorough clean up You can stop/remove all containers and images with this one-liner: docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q) && docker system prune -a If you need to remove dangling volumes, use this: docker volume prune","title":"Tips & Tricks"},{"location":"tips_and_tricks/#remove-unwanted-files","text":"Especially during development, it can happen that files get generated when running your docker container that can only be removed by the root user again. If you do not have admin permissions (e.g., via sudo ) on the machine then you can revert to using docker to remove them again: change into the directory with unwanted files and/or directories the following docker command will map the current directory to /opt/local : docker run -v `pwd`:/opt/local -it bash:5.1.8 change into /opt/local and remove all files/dirs (or just the files that need removing): cd /opt/local rm -Rf *","title":"Remove unwanted files"},{"location":"tips_and_tricks/#alternative-python-version","text":"In case your base Debian/Ubuntu distribution does not have the right Python version available, you can get additional versions by adding the deadsnakes ppa : add-apt-repository -y ppa:deadsnakes/ppa && \\ apt-get update","title":"Alternative Python version"},{"location":"tips_and_tricks/#switch-default-python-version","text":"If your base distro has an older version of Python and you do not want to sprinkle the specific newer version throughout your Dockerfile , then you can use update-alternatives on Debian systems to change the default. The following commands switch from Python 3.5 to Python 3.6 for the python3 executable and also use Python 3.6 as the default for the python executable: update-alternatives --install /usr/bin/python python /usr/bin/python3.5 1 && \\ update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2 && \\ update-alternatives --set python /usr/bin/python3.6 && \\ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 1 && \\ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 2 && \\ update-alternatives --set python3 /usr/bin/python3.6 && \\","title":"Switch default Python version"},{"location":"tips_and_tricks/#screen","text":"The screen command-line utility allows you to pick up remote sessions after detaching from them. This is especially helpful when running docker in interactive mode via ssh sessions. If such an ssh session should accidentally close (e.g., internet connection lost, laptop closed), then the docker command would terminate as well. On the remote host, start the screen command before you launch your docker command (you can run multiple screen instances as well) and skip the screen with the licenses etc using Enter or Space . Now you can start up the actual command. If you want to run multiple screen sessions on the same host, then you should name them using the -S sessionname option to easily distinguish them. For detaching a session, use the CTRL+A+D key combination. This will leave your process running in the background and you can close the remote connection. In order to reconnect , simply ssh into the remote host and run screen -r . If there is only one screen session running, then it will automatically reconnect. Otherwise, it will output a list of available sessions. Supply the name of the session that you want to reattach to the -r option. You can exit a screen session by either typing exit or using CTRL+D (just like with a bash shell).","title":"Screen"},{"location":"tips_and_tricks/#default-runtime","text":"Instead of always having to type --runtime=nvidia or --gpus=all , you can simply define the default runtime to use with your docker commands ( source ): edit the /etc/docker/daemon.json file as root user insert the key default-runtime with value nvidia so that it looks something like this: { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } } } save the file restart the docker daemon (e.g., sudo systemctl restart docker )","title":"Default runtime"},{"location":"tips_and_tricks/#thorough-clean-up","text":"You can stop/remove all containers and images with this one-liner: docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q) && docker system prune -a If you need to remove dangling volumes, use this: docker volume prune","title":"Thorough clean up"},{"location":"troubleshooting/","text":"CUDA not available at build time Some deep learning libraries refuse to get installed if there is no CUDA-capable device available. Of course, you can fix this by defining a default runtime in your docker configuration. However, this is not an option when your build system does not even have a GPU. Therefore, an alternative is to skip the CUDA device check by inserting the FORCE_CUDA environment variable in your docker build as follows: ENV FORCE_CUDA=\"1\" CUDA architectures for PyTorch If your container needs to build PyTorch (e.g., if you cannot use a PyTorch base image), then you can supply the list of NVIDIA architectures via the TORCH_CUDA_ARCH_LIST environment variable. By using ARG , you can define a default value and still override it at build time via the --build-arg option: ARG TORCH_CUDA_ARCH_LIST=\"Kepler;Kepler+Tesla;Maxwell;Maxwell+Tegra;Pascal;Volta;Turing\" ENV TORCH_CUDA_ARCH_LIST=\"${TORCH_CUDA_ARCH_LIST}\" Interactive timezone prompt In case you get the following prompt for configuring your timezone data (which will fail unattended builds): Configuring tzdata ------------------ Please select the geographic area in which you live. Subsequent configuration questions will narrow this down by presenting a list of cities, representing the time zones in which they are located. 1. Africa 4. Australia 7. Atlantic 10. Pacific 13. Etc 2. America 5. Arctic 8. Europe 11. SystemV 3. Antarctica 6. Asia 9. Indian 12. US Geographic area: Then you can precede your command with DEBIAN_FRONTEND=noninteractive . In case of apt-get , use something like this: RUN DEBIAN_FRONTEND=noninteractive apt-get install -y ... Or like this: RUN export DEBIAN_FRONTEND=noninteractive && \\ apt-get install -y ... Finally, the brute force method is to define it globally via the ENV command: ENV DEBIAN_FRONTEND noninteractive Missing shared library A common error that you will encounter is installations via pip failing due to a shared library not being present, similar to this: error: XYZ.so: cannot open shared object file: No such file or directory In order to remedy the problem, you need to determine the package that contains this shared library. In case of Ubuntu (or Debian) you can use the Ubuntu Packages Search : https://packages.ubuntu.com/ On that page, scroll to the Search the contents of packages section and enter the file name of the missing shared library (e.g., XYZ.so ). If you know the distribution (e.g., bionic release) and architecture (e.g., amd64 ) then you can restrict the search further. Finally, click on Search . From the results page, select the appropriate package name and include that in your docker build.","title":"Troubleshooting"},{"location":"troubleshooting/#cuda-not-available-at-build-time","text":"Some deep learning libraries refuse to get installed if there is no CUDA-capable device available. Of course, you can fix this by defining a default runtime in your docker configuration. However, this is not an option when your build system does not even have a GPU. Therefore, an alternative is to skip the CUDA device check by inserting the FORCE_CUDA environment variable in your docker build as follows: ENV FORCE_CUDA=\"1\"","title":"CUDA not available at build time"},{"location":"troubleshooting/#cuda-architectures-for-pytorch","text":"If your container needs to build PyTorch (e.g., if you cannot use a PyTorch base image), then you can supply the list of NVIDIA architectures via the TORCH_CUDA_ARCH_LIST environment variable. By using ARG , you can define a default value and still override it at build time via the --build-arg option: ARG TORCH_CUDA_ARCH_LIST=\"Kepler;Kepler+Tesla;Maxwell;Maxwell+Tegra;Pascal;Volta;Turing\" ENV TORCH_CUDA_ARCH_LIST=\"${TORCH_CUDA_ARCH_LIST}\"","title":"CUDA architectures for PyTorch"},{"location":"troubleshooting/#interactive-timezone-prompt","text":"In case you get the following prompt for configuring your timezone data (which will fail unattended builds): Configuring tzdata ------------------ Please select the geographic area in which you live. Subsequent configuration questions will narrow this down by presenting a list of cities, representing the time zones in which they are located. 1. Africa 4. Australia 7. Atlantic 10. Pacific 13. Etc 2. America 5. Arctic 8. Europe 11. SystemV 3. Antarctica 6. Asia 9. Indian 12. US Geographic area: Then you can precede your command with DEBIAN_FRONTEND=noninteractive . In case of apt-get , use something like this: RUN DEBIAN_FRONTEND=noninteractive apt-get install -y ... Or like this: RUN export DEBIAN_FRONTEND=noninteractive && \\ apt-get install -y ... Finally, the brute force method is to define it globally via the ENV command: ENV DEBIAN_FRONTEND noninteractive","title":"Interactive timezone prompt"},{"location":"troubleshooting/#missing-shared-library","text":"A common error that you will encounter is installations via pip failing due to a shared library not being present, similar to this: error: XYZ.so: cannot open shared object file: No such file or directory In order to remedy the problem, you need to determine the package that contains this shared library. In case of Ubuntu (or Debian) you can use the Ubuntu Packages Search : https://packages.ubuntu.com/ On that page, scroll to the Search the contents of packages section and enter the file name of the missing shared library (e.g., XYZ.so ). If you know the distribution (e.g., bionic release) and architecture (e.g., amd64 ) then you can restrict the search further. Finally, click on Search . From the results page, select the appropriate package name and include that in your docker build.","title":"Missing shared library"}]}