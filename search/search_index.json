{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"From one data scientist to another on how to utilize docker to make your life easier. Any data scientist that ever had to set up an environment for a deep learning framework, knows that getting the combination of CUDA, cuDNN, framework and other libraries right is a frustrating exercise. With docker you still have to go through the pain of figuring out the right combination, but... And it is a BIG but! Once you have this blueprint called docker image , you can use it on another machine; you will be up and running in seconds. This is by no means supposed to be an exhaustive introduction to docker (docker can do a lot more!), but merely for getting you started on your journey. To get started with docker, you should look at the following sections: Basics - explains the most common operations that you will need to know Dockerfile - explains the structure of the Dockerfile file which is used to create docker images Best practices - what to keep in mind when creating and using images Repositories - where to find the base images that your own images will use If you should encounter problems, then have a look here: Troubleshooting Just like with any programming language or complex framework, there are certain things that can make your life easier. Therefore do not forget to have a look at: Tips & Tricks Once you got a handle on things, and you are getting tired on manually building images, you might want to look into automating your builds and maybe also run your own registry/proxy. In that case, have a look at the following sections: Automation Registry","title":"Home"},{"location":"automation/","text":"Building docker images by hand is fine when you are developing a new image. However, once the development phase is over and maintenance starts, you will only make minor changes. Having to build and push out an image then becomes tedious. Instead, automate the build and push process using a build system. Of course, there are a million and one different build tools out there and you will have to find the one that you like best and that fits your environment. Coming from a Java/Maven background, I was already familiar with Jenkins . It may not be the best of interfaces, but if you are only occasionally performing builds then it is nice if you can just parametrize some plugins through a web interface and concentrate on other tasks. For my docker builds, I use the Docker plugin for Jenkins . For publicly images hosted on public registries that use public git repositories, the plugin is really easy to use. Using Jenkins' credential mechanism, you can also push out images to private registries (for cloning/pulling from private git repositories, you can employ some ssh-config magic ). Other (untested) approaches you could look into are: gradle github actions","title":"Automation"},{"location":"basics/","text":"login credentials (managers) pulling running (named/unnamed) stopping cleaning up interactive vs non-interactive gpu support volumes","title":"Basics"},{"location":"best_practices/","text":"Versioning Since docker determines by the hash of a command whether this particular layer needs rebuilding, you should, wherever possible, specify the version of the library that you are installing. That way, whenever you upgrade a library, that layer will get rebuilt. The added bonus is that you will not accidentally rebuild the image with a newer version of a library at a later stage that may now be incompatible with the rest ( Yeah, I'm looking at you, numpy !). So instead of: RUN python3 -m pip install --no-cache-dir scikit-image && \\ ... You should do something like this: RUN python3 -m pip install --no-cache-dir scikit-image==0.16.2 && \\ ... Of course, when you are cloning directly from a github repository, because you require a specific bugfix or the library does not offer any releases (or only very infrequent), then you should use a specific commit hash in your command: RUN git clone https://github.com/ACCOUNT/REPO.git && \\ cd REPO && \\ git reset --hard 11223344556677889900AABBCCDDEEFF11223344 && \\ ... Docker group Instead of running the docker command via sudo , you should consider adding your user to the docker group instead (in /etc/group ). That way, you can run the docker command as a regular user. Launch container as regular user Once development of your docker image has finished, you should avoid running docker containers as the root user and instead run it as the current user (which also avoids creating output files in volumes that can only be removed by root ). Use the -u and -e command to specify the user/group IDs and the user name as follows: docker run -u $(id -u):$(id -g) -e USER=$USER ... However, the environment variables for you command prompt (when you are using your container in interactive mode) may not be able to handle this properly. In such a case you will get output similar to this: groups: cannot find name for group ID XYZ I have no name!@c7355d9a1b59:/$ You can rectify this by creating a custom bash.bashrc file using the docker-banner-gen library. This custom file can then be added to your docker image using the following command: COPY bash.bashrc /etc/bash.bashrc No Anaconda Instead of using Anaconda for installing Python packages, you can just use plain pip to install packages from the Python Package Index . Anaconda in itself is quite a large installation and will increase the overall docker image size unnecessarily. Clean up pip Remove the pip cache to reduce the size of your layer: rm -Rf /root/.cache/pip Alternatively, do not cache downloads: pip install --no-cache-dir ... Clean up apt After performing installs via apt or apt-get, clean up the cache to reduce the size of your layer: apt-get clean && \\ rm -rf /var/lib/apt/lists/* Caching models As soon as you stop/remove the container, all modifications will be lost. This also includes any pretrained networks that any of the deep learning frameworks downloaded into its cache. In order to avoid the constant downloads, you can map the cache directories for the relevant framework to a local directory on the host: PyTorch when running the container as root -v /some/where/cache:/root/.torch \\ when running the container as current user -v /some/where/cache:/.torch \\ Keras Map cache directory (/root/.keras) when running container as root: -v /some/where/cache:/root/.keras Or when running as regular user (/tmp/.keras): -v /some/where/cache:/tmp/.keras","title":"Best practices"},{"location":"best_practices/#versioning","text":"Since docker determines by the hash of a command whether this particular layer needs rebuilding, you should, wherever possible, specify the version of the library that you are installing. That way, whenever you upgrade a library, that layer will get rebuilt. The added bonus is that you will not accidentally rebuild the image with a newer version of a library at a later stage that may now be incompatible with the rest ( Yeah, I'm looking at you, numpy !). So instead of: RUN python3 -m pip install --no-cache-dir scikit-image && \\ ... You should do something like this: RUN python3 -m pip install --no-cache-dir scikit-image==0.16.2 && \\ ... Of course, when you are cloning directly from a github repository, because you require a specific bugfix or the library does not offer any releases (or only very infrequent), then you should use a specific commit hash in your command: RUN git clone https://github.com/ACCOUNT/REPO.git && \\ cd REPO && \\ git reset --hard 11223344556677889900AABBCCDDEEFF11223344 && \\ ...","title":"Versioning"},{"location":"best_practices/#docker-group","text":"Instead of running the docker command via sudo , you should consider adding your user to the docker group instead (in /etc/group ). That way, you can run the docker command as a regular user.","title":"Docker group"},{"location":"best_practices/#launch-container-as-regular-user","text":"Once development of your docker image has finished, you should avoid running docker containers as the root user and instead run it as the current user (which also avoids creating output files in volumes that can only be removed by root ). Use the -u and -e command to specify the user/group IDs and the user name as follows: docker run -u $(id -u):$(id -g) -e USER=$USER ... However, the environment variables for you command prompt (when you are using your container in interactive mode) may not be able to handle this properly. In such a case you will get output similar to this: groups: cannot find name for group ID XYZ I have no name!@c7355d9a1b59:/$ You can rectify this by creating a custom bash.bashrc file using the docker-banner-gen library. This custom file can then be added to your docker image using the following command: COPY bash.bashrc /etc/bash.bashrc","title":"Launch container as regular user"},{"location":"best_practices/#no-anaconda","text":"Instead of using Anaconda for installing Python packages, you can just use plain pip to install packages from the Python Package Index . Anaconda in itself is quite a large installation and will increase the overall docker image size unnecessarily.","title":"No Anaconda"},{"location":"best_practices/#clean-up-pip","text":"Remove the pip cache to reduce the size of your layer: rm -Rf /root/.cache/pip Alternatively, do not cache downloads: pip install --no-cache-dir ...","title":"Clean up pip"},{"location":"best_practices/#clean-up-apt","text":"After performing installs via apt or apt-get, clean up the cache to reduce the size of your layer: apt-get clean && \\ rm -rf /var/lib/apt/lists/*","title":"Clean up apt"},{"location":"best_practices/#caching-models","text":"As soon as you stop/remove the container, all modifications will be lost. This also includes any pretrained networks that any of the deep learning frameworks downloaded into its cache. In order to avoid the constant downloads, you can map the cache directories for the relevant framework to a local directory on the host: PyTorch when running the container as root -v /some/where/cache:/root/.torch \\ when running the container as current user -v /some/where/cache:/.torch \\ Keras Map cache directory (/root/.keras) when running container as root: -v /some/where/cache:/root/.keras Or when running as regular user (/tmp/.keras): -v /some/where/cache:/tmp/.keras","title":"Caching models"},{"location":"dockerfile/","text":"FROM ARG RUN COPY ADD optimizing dockerfile","title":"Dockerfile"},{"location":"registry/","text":"If you have to manage internal images as well, that should not be available from the outside (e.g., via docker hub), then you should consider running your own in-house registry. If your setup requires access control or you just want to have a web interface, then I can recommend Sonatype's Nexus: https://www.sonatype.com/products/repository-oss NB: The open-source version is absolutely sufficient for this purpose ( OSS vs Pro ). The biggest advantage of having a local Nexus instance is, that you can use it as a proxy for docker hub. That way, you can pull base images from your local network rather than having to pull them from a distant server. This makes rebuilding images very fast, especially if you just removed all images from your machine or moved to a new machine.","title":"Registry"},{"location":"repos/","text":"A lot of open-source deep learning projects will supply docker base images already on docker hub , which will make your life much easier. By using these base images, you will only need to install any additional libraries that your project requires. Here is a (non-exhaustive) list of repositories on docker hub: Ubuntu CUDA PyTorch Tensorflow Of course, NVIDIA likes to keep things close to its heart and offers their own registry from which you can also retrieve images for the various IoT devices that NVIDIA has on offer: NGC catalog NVIDIA L4T Base NVIDIA L4T PyTorch NVIDIA L4T Tensorflow","title":"Repositories"},{"location":"tips_and_tricks/","text":"Remove unwanted files Especially during development, it can happen that files get generated when running your docker container that can only be removed by the root user again. If you do not have admin permissions (e.g., via sudo ) on the machine then you can revert to using docker to remove them again: change into the directory with unwanted files and/or directories the following docker command will map the current directory to /opt/local : docker run -v `pwd`:/opt/local -it public.aml-repo.cms.waikato.ac.nz:443/bash:5.1.8 change into /opt/local and remove all files/dirs (or just the files that need removing): cd /opt/local rm -Rf * Alternative Python version In case your base Debian/Ubuntu distribution does not have the right Python version available, you can get additional versions by adding the deadsnakes ppa : add-apt-repository -y ppa:deadsnakes/ppa && \\ apt-get update Switch default Python version If your base distro has an older version of Python and you do not want to sprinkle that specific version throughout your Dockerfile , then you can use update-alternatives on Debian systems to change the default. The following commands switch from Python 3.5 to Python 3.6 for the python3 executable and also use Python 3.6 as the default for the python executable: update-alternatives --install /usr/bin/python python /usr/bin/python3.5 1 && \\ update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2 && \\ update-alternatives --set python /usr/bin/python3.6 && \\ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 1 && \\ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 2 && \\ update-alternatives --set python3 /usr/bin/python3.6 && \\ Screen The screen command-line utility allows you to pick up remote sessions after detaching from them. This is especially helpful when running docker in interactive mode via ssh sessions. If such an ssh session should accidentally close (e.g., internet connection lost, laptop closed), then the docker command would terminate as well. On the remote host, start the screen command before your docker command (you can run multiple screen instances as well) and skip the screen with the licenses etc using Enter or Space . Now you can start up the actual command. If you want to run multiple screen sessions on the same host, then you should name them using the -S sessionname option. For detaching , use the CTRL+A+D key combination. This will leave your process running in the background and you can close the remote connection. In order to reconnect , simply ssh into the remote host and run screen -r . If there is only one screen session running, then it will automatically reconnect. Otherwise, it will output a list of available sessions. Supply the name of the session that you want to reattach to the -r option. You can exit a screen session by either typing exit or using CTRL+D (just like with bash ). Default runtime Instead of always having to type --runtime=nvidia or --gpus=all , you simply define the default runtime to use with your docker commands ( source ): edit the /etc/docker/daemon.json file insert key default-runtime with value nvidia so that it looks something like this: { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } } } save the file Thorough clean up You can stop/remove all containers and images with this one-liner: docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q) && docker system prune -a If you need to remove dangling volumes, use this: docker volume prune","title":"Tips & Tricks"},{"location":"tips_and_tricks/#remove-unwanted-files","text":"Especially during development, it can happen that files get generated when running your docker container that can only be removed by the root user again. If you do not have admin permissions (e.g., via sudo ) on the machine then you can revert to using docker to remove them again: change into the directory with unwanted files and/or directories the following docker command will map the current directory to /opt/local : docker run -v `pwd`:/opt/local -it public.aml-repo.cms.waikato.ac.nz:443/bash:5.1.8 change into /opt/local and remove all files/dirs (or just the files that need removing): cd /opt/local rm -Rf *","title":"Remove unwanted files"},{"location":"tips_and_tricks/#alternative-python-version","text":"In case your base Debian/Ubuntu distribution does not have the right Python version available, you can get additional versions by adding the deadsnakes ppa : add-apt-repository -y ppa:deadsnakes/ppa && \\ apt-get update","title":"Alternative Python version"},{"location":"tips_and_tricks/#switch-default-python-version","text":"If your base distro has an older version of Python and you do not want to sprinkle that specific version throughout your Dockerfile , then you can use update-alternatives on Debian systems to change the default. The following commands switch from Python 3.5 to Python 3.6 for the python3 executable and also use Python 3.6 as the default for the python executable: update-alternatives --install /usr/bin/python python /usr/bin/python3.5 1 && \\ update-alternatives --install /usr/bin/python python /usr/bin/python3.6 2 && \\ update-alternatives --set python /usr/bin/python3.6 && \\ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.5 1 && \\ update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.6 2 && \\ update-alternatives --set python3 /usr/bin/python3.6 && \\","title":"Switch default Python version"},{"location":"tips_and_tricks/#screen","text":"The screen command-line utility allows you to pick up remote sessions after detaching from them. This is especially helpful when running docker in interactive mode via ssh sessions. If such an ssh session should accidentally close (e.g., internet connection lost, laptop closed), then the docker command would terminate as well. On the remote host, start the screen command before your docker command (you can run multiple screen instances as well) and skip the screen with the licenses etc using Enter or Space . Now you can start up the actual command. If you want to run multiple screen sessions on the same host, then you should name them using the -S sessionname option. For detaching , use the CTRL+A+D key combination. This will leave your process running in the background and you can close the remote connection. In order to reconnect , simply ssh into the remote host and run screen -r . If there is only one screen session running, then it will automatically reconnect. Otherwise, it will output a list of available sessions. Supply the name of the session that you want to reattach to the -r option. You can exit a screen session by either typing exit or using CTRL+D (just like with bash ).","title":"Screen"},{"location":"tips_and_tricks/#default-runtime","text":"Instead of always having to type --runtime=nvidia or --gpus=all , you simply define the default runtime to use with your docker commands ( source ): edit the /etc/docker/daemon.json file insert key default-runtime with value nvidia so that it looks something like this: { \"default-runtime\": \"nvidia\", \"runtimes\": { \"nvidia\": { \"path\": \"nvidia-container-runtime\", \"runtimeArgs\": [] } } } save the file","title":"Default runtime"},{"location":"tips_and_tricks/#thorough-clean-up","text":"You can stop/remove all containers and images with this one-liner: docker stop $(docker ps -a -q) && docker rm $(docker ps -a -q) && docker system prune -a If you need to remove dangling volumes, use this: docker volume prune","title":"Thorough clean up"},{"location":"troubleshooting/","text":"CUDA not available at build time Some deep learning libraries refuse to get installed if there is not CUDA-capable device available. Of course, you can fix this by defining a default runtime in your docker configuration. However, this is not an option when your build system does not even have a GPU. Therefore, an alternative is to skip the CUDA device check by inserting the FORCE_CUDA environment variable in your docker build as follows: ENV FORCE_CUDA=\"1\" CUDA architectures for PyTorch If your container needs to build PyTorch (e.g., if you cannot use a PyTorch base image), then you can supply the list of NVIDIA architectures via the TORCH_CUDA_ARCH_LIST environment variable. By using ARG , you can define a default valye and still override it at build time via the --build-arg option: ARG TORCH_CUDA_ARCH_LIST=\"Kepler;Kepler+Tesla;Maxwell;Maxwell+Tegra;Pascal;Volta;Turing\" ENV TORCH_CUDA_ARCH_LIST=\"${TORCH_CUDA_ARCH_LIST}\" Interactive timezone prompt In case you get the following prompt for configuring your timezone data (which will fail unattended builds): Configuring tzdata ------------------ Please select the geographic area in which you live. Subsequent configuration questions will narrow this down by presenting a list of cities, representing the time zones in which they are located. 1. Africa 4. Australia 7. Atlantic 10. Pacific 13. Etc 2. America 5. Arctic 8. Europe 11. SystemV 3. Antarctica 6. Asia 9. Indian 12. US Geographic area: Then you can precede your command with DEBIAN_FRONTEND=noninteractive . In case of apt-get , use something like this: RUN DEBIAN_FRONTEND=noninteractive apt-get install -y ... Or like this: RUN export DEBIAN_FRONTEND=noninteractive && \\ apt-get install -y ... Finally, the brute force method is to define it globally via the ENV command: ENV DEBIAN_FRONTEND noninteractive Missing shared library A common error that you will encounter is installations via pip failing to a shared library not being present, similar to this: error: XYZ.so: cannot open shared object file: No such file or directory In order to remedy the problem, you need to determine the package that contains this shared library. In case of Ubuntu (or Debian) you can use the Ubuntu Packages Search : https://packages.ubuntu.com/ On that page, scroll to the Search the contents of packages section and enter the file name of the missing shared library (e.g., XYZ.so ). If you know the distribution (e.g., bionic release) and architecture (e.g., amd64 ) then you can restrict the search further. Finally, click on Search . From the results page, select the appropriate package name and include that in your docker build.","title":"Troubleshooting"},{"location":"troubleshooting/#cuda-not-available-at-build-time","text":"Some deep learning libraries refuse to get installed if there is not CUDA-capable device available. Of course, you can fix this by defining a default runtime in your docker configuration. However, this is not an option when your build system does not even have a GPU. Therefore, an alternative is to skip the CUDA device check by inserting the FORCE_CUDA environment variable in your docker build as follows: ENV FORCE_CUDA=\"1\"","title":"CUDA not available at build time"},{"location":"troubleshooting/#cuda-architectures-for-pytorch","text":"If your container needs to build PyTorch (e.g., if you cannot use a PyTorch base image), then you can supply the list of NVIDIA architectures via the TORCH_CUDA_ARCH_LIST environment variable. By using ARG , you can define a default valye and still override it at build time via the --build-arg option: ARG TORCH_CUDA_ARCH_LIST=\"Kepler;Kepler+Tesla;Maxwell;Maxwell+Tegra;Pascal;Volta;Turing\" ENV TORCH_CUDA_ARCH_LIST=\"${TORCH_CUDA_ARCH_LIST}\"","title":"CUDA architectures for PyTorch"},{"location":"troubleshooting/#interactive-timezone-prompt","text":"In case you get the following prompt for configuring your timezone data (which will fail unattended builds): Configuring tzdata ------------------ Please select the geographic area in which you live. Subsequent configuration questions will narrow this down by presenting a list of cities, representing the time zones in which they are located. 1. Africa 4. Australia 7. Atlantic 10. Pacific 13. Etc 2. America 5. Arctic 8. Europe 11. SystemV 3. Antarctica 6. Asia 9. Indian 12. US Geographic area: Then you can precede your command with DEBIAN_FRONTEND=noninteractive . In case of apt-get , use something like this: RUN DEBIAN_FRONTEND=noninteractive apt-get install -y ... Or like this: RUN export DEBIAN_FRONTEND=noninteractive && \\ apt-get install -y ... Finally, the brute force method is to define it globally via the ENV command: ENV DEBIAN_FRONTEND noninteractive","title":"Interactive timezone prompt"},{"location":"troubleshooting/#missing-shared-library","text":"A common error that you will encounter is installations via pip failing to a shared library not being present, similar to this: error: XYZ.so: cannot open shared object file: No such file or directory In order to remedy the problem, you need to determine the package that contains this shared library. In case of Ubuntu (or Debian) you can use the Ubuntu Packages Search : https://packages.ubuntu.com/ On that page, scroll to the Search the contents of packages section and enter the file name of the missing shared library (e.g., XYZ.so ). If you know the distribution (e.g., bionic release) and architecture (e.g., amd64 ) then you can restrict the search further. Finally, click on Search . From the results page, select the appropriate package name and include that in your docker build.","title":"Missing shared library"}]}